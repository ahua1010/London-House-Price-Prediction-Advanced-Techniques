{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Score 185091\n",
    "\n",
    "MAE: 109640.8983, RMSE: 538318.4967, R²: 0.8215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 修改 baseline 後還沒跑出結果\n",
    "- 使用TimeSeriesSplit，專門用於時間序列資料的交叉驗證方法，它的設計目的是為了模擬「未來不能影響過去」的情況，防止資料洩漏（data leakage）\n",
    "- 新增價格/面積等「密度型衍生特徵」\n",
    "- 處理稀有類別 + 平滑 Target Encoding\n",
    "- 用 LightGBM 作為 HybridModel 的機器學習部分，並用 Optuna 自動搜尋最佳超參數\n",
    "- Clip 房價 + Winsorize 特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:00:21.704201Z",
     "iopub.status.busy": "2025-06-19T11:00:21.703537Z",
     "iopub.status.idle": "2025-06-19T11:00:21.761597Z",
     "shell.execute_reply": "2025-06-19T11:00:21.760978Z",
     "shell.execute_reply.started": "2025-06-19T11:00:21.704179Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "倫敦房價預測 - 混合模型（趨勢分析 + 機器學習）\n",
    "使用時間序列特徵結合機器學習進行房價預測\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier\n",
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV,TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from catboost import CatBoostRegressor\n",
    "import optuna\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "\n",
    "def objective(trial, X, y, trend_cols, machine_cols, all_columns):\n",
    "    params = {\n",
    "        \"device\": \"gpu\", \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 6000, step=500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"random_state\": 42,\n",
    "        \"verbose\": -1\n",
    "    }\n",
    "\n",
    "    # ➡ 將 Encoder 與 HybridModel 串成管線\n",
    "    model = Pipeline([\n",
    "        ('Encoder', CustomEncoder()),\n",
    "        ('Model', HybridModel(\n",
    "            trend_model=Ridge(alpha=0.1),\n",
    "            machine_model=LGBMRegressor(**params),\n",
    "            trend_cols=trend_cols,\n",
    "            machine_cols=machine_cols,\n",
    "            all_columns=all_columns\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    score = cross_val_score(model, X, y,\n",
    "                            cv=tscv,\n",
    "                            scoring='neg_mean_absolute_error',\n",
    "                            error_score='raise')   # 方便除錯\n",
    "    return -score.mean()\n",
    "\n",
    "def create_time_features(data_list):\n",
    "    \"\"\"創建時間相關特徵\"\"\"\n",
    "    print(\"創建時間特徵...\")\n",
    "    for data in data_list:\n",
    "        # 創建時間索引\n",
    "        data['time'] = pd.to_datetime(dict(\n",
    "            year=data['sale_year'], \n",
    "            month=data['sale_month'], \n",
    "            day=15\n",
    "        ))\n",
    "        data['time'] = data['time'].dt.to_period('M')\n",
    "        \n",
    "        # 創建數值型時間特徵\n",
    "        data['time_numeric'] = (\n",
    "            (data['time'].dt.to_timestamp() - data['time'].min().to_timestamp()) / \n",
    "            np.timedelta64(1, 'D')\n",
    "        )\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "\n",
    "def preprocess_address_features(data_list):\n",
    "    \"\"\"處理地址相關特徵\"\"\"\n",
    "    print(\"處理地址特徵...\")\n",
    "    for data in data_list:\n",
    "        # 提取街道資訊\n",
    "        data['street'] = data['fullAddress'].apply(\n",
    "            lambda address: ' '.join(address.split(',')[-3].split(' ')[-2:])\n",
    "        )\n",
    "        \n",
    "        # 處理郵遞區號\n",
    "        data['postcode'] = data['postcode'].apply(\n",
    "            lambda postcode: postcode.split(' ')[1]\n",
    "        )\n",
    "        \n",
    "        # 移除國家欄位（所有資料都是同一個國家）\n",
    "        data.drop('country', axis=1, inplace=True)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "def engineer_address_features(data_list):\n",
    "    \"\"\"從 fullAddress 中提取更豐富的特徵\"\"\"\n",
    "    print(\"進行高級地址特徵工程...\")\n",
    "    \n",
    "    for df in data_list:\n",
    "        # 將地址轉為小寫以便搜索\n",
    "        address_lower = df['fullAddress'].str.lower()\n",
    "\n",
    "        # 1. 提取街道類型\n",
    "        df['street_type'] = address_lower.str.extract(r'\\b(road|street|avenue|lane|square|drive|court|place|gardens|mews)\\b', expand=False).fillna('unknown')\n",
    "\n",
    "        # 2. 是否為公寓/樓層\n",
    "        df['is_flat_or_apt'] = address_lower.str.contains(r'flat|apartment|unit|floor|level').astype(int)\n",
    "\n",
    "        # 3. 提取數字資訊 (可能代表門牌號或公寓號)\n",
    "        # 提取第一個出現的數字序列\n",
    "        df['address_number'] = address_lower.str.extract(r'(\\d+)').astype(float).fillna(0)\n",
    "\n",
    "        # 4. 地址長度 (一個代理特徵，更長的地址可能意味著更複雜的建築)\n",
    "        df['address_length'] = df['fullAddress'].str.len()\n",
    "\n",
    "        # 5. 關鍵詞計數\n",
    "        keywords = ['mansion', 'penthouse', 'cottage', 'studio', 'garden', 'park', 'view', 'river', 'new build']\n",
    "        for keyword in keywords:\n",
    "            df[f'has_{keyword}'] = address_lower.str.contains(keyword).astype(int)\n",
    "            \n",
    "    return data_list\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    計算兩點之間的 haversine 距離（單位：公里）\n",
    "    \"\"\"\n",
    "    R = 6371  # 地球半徑 (km)\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    return R * 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "# ==================== 全新的、安全的目標編碼核心函式 ====================\n",
    "\n",
    "def create_safe_target_encoding(train_df, test_df, column_to_encode, target_series):\n",
    "    \"\"\"\n",
    "    使用 TimeSeriesSplit 進行安全的目標編碼，防止數據洩漏。\n",
    "    \"\"\"\n",
    "    print(f\"    -> 安全目標編碼: {column_to_encode}\")\n",
    "    \n",
    "    # 初始化空的特徵欄位\n",
    "    train_df[f'{column_to_encode}_mean_price'] = np.nan\n",
    "    train_df[f'{column_to_encode}_price_volatility'] = np.nan\n",
    "    \n",
    "    # 使用時間序列交叉驗證來生成訓練集的編碼\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    for train_index, val_index in tscv.split(train_df):\n",
    "        # 準備當前折的訓練數據和驗證數據\n",
    "        train_fold = train_df.iloc[train_index]\n",
    "        val_fold = train_df.iloc[val_index]\n",
    "        target_train_fold = target_series.iloc[train_index]\n",
    "        \n",
    "        # 在訓練折上計算編碼\n",
    "        encoding = train_fold.join(target_train_fold.rename('target')).groupby(column_to_encode)['target'].agg(['mean', 'std'])\n",
    "        mean_map = encoding['mean'].to_dict()\n",
    "        std_map = encoding['std'].fillna(0).to_dict()\n",
    "        \n",
    "        # 將計算出的編碼應用到驗證折上\n",
    "        train_df.loc[val_fold.index, f'{column_to_encode}_mean_price'] = val_fold[column_to_encode].map(mean_map)\n",
    "        train_df.loc[val_fold.index, f'{column_to_encode}_price_volatility'] = val_fold[column_to_encode].map(std_map)\n",
    "        \n",
    "    # --- 為整個訓練集和測試集計算全局編碼 ---\n",
    "    # 全局編碼用於填充交叉驗證中可能產生的缺失值，以及為測試集編碼\n",
    "    global_encoding = train_df.join(target_series.rename('target')).groupby(column_to_encode)['target'].agg(['mean', 'std'])\n",
    "    global_mean_map = global_encoding['mean'].to_dict()\n",
    "    global_std_map = global_encoding['std'].fillna(0).to_dict()\n",
    "    \n",
    "    # 填充訓練集中可能因新類別產生的 NaN\n",
    "    train_df[f'{column_to_encode}_mean_price'].fillna(target_series.mean(), inplace=True)\n",
    "    train_df[f'{column_to_encode}_price_volatility'].fillna(target_series.std(), inplace=True)\n",
    "    \n",
    "    # 應用全局編碼到測試集\n",
    "    test_df[f'{column_to_encode}_mean_price'] = test_df[column_to_encode].map(global_mean_map).fillna(target_series.mean())\n",
    "    test_df[f'{column_to_encode}_price_volatility'] = test_df[column_to_encode].map(global_std_map).fillna(target_series.std())\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# ==================== 請用這個簡化版，替換舊的地理特徵函式 ====================\n",
    "\n",
    "def engineer_geo_features(train_df, test_df):\n",
    "    \"\"\"\n",
    "    執行地理特徵工程（已移除所有價格相關特徵，只保留純幾何特徵）。\n",
    "    \"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    print(\"-> 開始執行地理特徵工程（僅純幾何特徵）...\")\n",
    "    \n",
    "    train_df_copy = train_df.copy()\n",
    "    test_df_copy = test_df.copy()\n",
    "    \n",
    "    for df in [train_df_copy, test_df_copy]:\n",
    "        # === 1. 基本地理數學特徵 ===\n",
    "        df['lat_lon_ratio'] = df['latitude'] / (df['longitude'] + 1e-9)\n",
    "        df['lat_lon_product'] = df['latitude'] * df['longitude']\n",
    "\n",
    "        # === 2. 倫敦重要地標距離特徵 ===\n",
    "        london_landmarks = {\n",
    "            'city_center': (51.5074, -0.1278), 'canary_wharf': (51.5055, -0.0195),\n",
    "            'westminster': (51.4994, -0.1244), 'heathrow': (51.4700, -0.4543)\n",
    "        }\n",
    "        for name, (lat, lon) in london_landmarks.items():\n",
    "            df[f'dist_to_{name}'] = haversine_distance(df['latitude'], df['longitude'], lat, lon)\n",
    "\n",
    "    # === 3. 地理聚類特徵 (Fit on train, transform both) ===\n",
    "    # 這一步是安全的，因為它只基於座標，不涉及價格\n",
    "    coords_train = train_df_copy[['latitude', 'longitude']].values\n",
    "    coords_test = test_df_copy[['latitude', 'longitude']].values\n",
    "    cluster_configs = {'geo_cluster_medium': 20, 'geo_cluster_fine': 50}\n",
    "    \n",
    "    for name, k in cluster_configs.items():\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        train_df_copy[name] = kmeans.fit_predict(coords_train)\n",
    "        test_df_copy[name] = kmeans.predict(coords_test)\n",
    "        \n",
    "    print(\"-> 純地理特徵工程完成。\")\n",
    "    return train_df_copy, test_df_copy\n",
    "\n",
    "def impute_missing_values_with_strategy(data_list, column_name, strategy='most_frequent'):\n",
    "    \"\"\"使用指定策略填補缺失值\"\"\"\n",
    "    print(f\"填補 {column_name} 的缺失值（策略：{strategy}）...\")\n",
    "    \n",
    "    # 從訓練資料學習填補策略\n",
    "    train_data = data_list[0]  # 第一個是訓練資料\n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    imputer.fit(train_data[[column_name]])\n",
    "    \n",
    "    # 對所有資料集應用填補\n",
    "    for data in data_list:\n",
    "        data[column_name] = imputer.transform(data[[column_name]]).ravel()\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "\n",
    "def impute_with_regression(data_list, target_column, feature_column):\n",
    "    \"\"\"使用回歸模型填補缺失值\"\"\"\n",
    "    print(f\"使用 {feature_column} 預測填補 {target_column} 的缺失值...\")\n",
    "    \n",
    "    train_data = data_list[0]\n",
    "    test_data = data_list[1]\n",
    "    \n",
    "    # 準備完整的訓練資料\n",
    "    complete_train_data = train_data.dropna(subset=[target_column, feature_column])\n",
    "    X_train = complete_train_data[[feature_column]]\n",
    "    y_train = complete_train_data[target_column]\n",
    "    \n",
    "    # 訓練回歸模型\n",
    "    regression_model = Ridge()\n",
    "    regression_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 填補訓練集的缺失值\n",
    "    missing_train_mask = train_data[target_column].isna()\n",
    "    if missing_train_mask.any():\n",
    "        missing_train_features = train_data.loc[missing_train_mask, [feature_column]]\n",
    "        train_data.loc[missing_train_mask, target_column] = regression_model.predict(missing_train_features)\n",
    "    \n",
    "    # 填補測試集的缺失值\n",
    "    missing_test_mask = test_data[target_column].isna()\n",
    "    if missing_test_mask.any():\n",
    "        missing_test_features = test_data.loc[missing_test_mask, [feature_column]]\n",
    "        test_data.loc[missing_test_mask, target_column] = regression_model.predict(missing_test_features)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "def handle_missing_values(data_list):\n",
    "    \"\"\"處理所有缺失值\"\"\"\n",
    "    print(\"開始處理缺失值...\")\n",
    "    \n",
    "    # 使用最頻繁值填補面積\n",
    "    data_list = impute_missing_values_with_strategy(data_list, 'floorAreaSqM')\n",
    "    \n",
    "    # 使用面積預測浴室數量\n",
    "    data_list = impute_with_regression(data_list, 'bathrooms', 'floorAreaSqM')\n",
    "    \n",
    "    # 使用面積預測臥室數量\n",
    "    data_list = impute_with_regression(data_list, 'bedrooms', 'floorAreaSqM')\n",
    "    \n",
    "    # 使用最頻繁值填補其他類別特徵\n",
    "    categorical_columns = ['livingRooms', 'tenure', 'propertyType', 'currentEnergyRating']\n",
    "    for column in categorical_columns:\n",
    "        data_list = impute_missing_values_with_strategy(data_list, column)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "\n",
    "def create_time_series_features(train_data, test_data):\n",
    "    \"\"\"創建時間序列特徵\"\"\"\n",
    "    print(\"創建時間序列特徵...\")\n",
    "    \n",
    "    # 創建確定性過程（趨勢、季節性、週期性）\n",
    "    deterministic_process = DeterministicProcess(\n",
    "        index=train_data.index.unique(),\n",
    "        constant=True,        # 常數項\n",
    "        seasonal=True,        # 季節性\n",
    "        order=12,            # 趨勢階數\n",
    "        drop=True,           # 移除共線性\n",
    "        additional_terms=[CalendarFourier(freq=\"QE\", order=4)],  # 季度傅立葉項\n",
    "    )\n",
    "    \n",
    "    # 為訓練資料添加時間序列特徵\n",
    "    time_features_train = deterministic_process.in_sample()\n",
    "    train_data = train_data.join(time_features_train, how='left')\n",
    "    \n",
    "    # 計算預測相關參數\n",
    "    forecast_origin = train_data.index.max()\n",
    "    forecast_lead = test_data.index.min() - forecast_origin\n",
    "    forecast_horizon = test_data.index.max() - test_data.index.min()\n",
    "    \n",
    "    print(f\"預測起點: {forecast_origin}\")\n",
    "    print(f\"領先時間: {forecast_lead.n} 個月\")\n",
    "    print(f\"預測範圍: {forecast_horizon.n} 個月\")\n",
    "    \n",
    "    # 為測試資料添加時間序列特徵\n",
    "    time_features_test = deterministic_process.out_of_sample(\n",
    "        steps=forecast_horizon.n + forecast_lead.n\n",
    "    )\n",
    "    test_data = test_data.join(time_features_test, how='left')\n",
    "    test_data.index.name = 'time'\n",
    "    \n",
    "    return train_data, test_data, time_features_train.columns.tolist()\n",
    "\n",
    "\n",
    "def create_additional_features(data_list):\n",
    "    \"\"\"創建額外的特徵\"\"\"\n",
    "    print(\"創建額外特徵...\")\n",
    "    \n",
    "    for data in data_list:\n",
    "        # 總房間數 = 臥室 + 起居室\n",
    "        data['rooms'] = data['bedrooms'] + data['livingRooms']\n",
    "        # 總房間數 = 臥室 + 起居室\n",
    "        data['rooms'] = data['bedrooms'] + data['livingRooms']\n",
    "        \n",
    "        # 衍生密度特徵\n",
    "        data['rooms_per_bedroom'] = data['rooms'] / np.maximum(data['bedrooms'], 1)\n",
    "        data['bath_per_room'] = data['bathrooms'] / np.maximum(data['rooms'], 1)\n",
    "\n",
    "        # === 新增：創建交叉特徵 ===\n",
    "        # 將 outcode 和 propertyType 結合，形成更具體的特徵\n",
    "        # 例如 \"SW1_Flat\" (SW1區的公寓)\n",
    "        data['outcode_proptype'] = data['outcode'].astype(str) + \"_\" + data['propertyType'].astype(str)\n",
    "\n",
    "        # 將 outcode 和 tenure 結合\n",
    "        data['outcode_tenure'] = data['outcode'].astype(str) + \"_\" + data['tenure'].astype(str)\n",
    "            \n",
    "    \n",
    "    return data_list\n",
    "\n",
    "class CustomEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.target_mean_encoders = {}\n",
    "        self.fallback_values = {}\n",
    "        self.bin_encoders = {}\n",
    "        self.ordinal_encoders = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        X_copy['price'] = y\n",
    "        \n",
    "        # 簡單目標編碼 (不處理稀有值，不平滑)\n",
    "        target_encoding_features =  [\n",
    "            'street', 'postcode', 'outcode', 'tenure', 'propertyType',\n",
    "            'street_type', 'outcode_proptype', 'outcode_tenure',\n",
    "        \n",
    "            # vvvvvvvvvvvv 新增需要編碼的地理特徵 vvvvvvvvvvvv\n",
    "            'geo_cluster_medium', 'geo_cluster_fine'\n",
    "        ]\n",
    "        for feature in target_encoding_features:\n",
    "            self.target_mean_encoders[feature] = X_copy.groupby(feature)['price'].mean()\n",
    "            self.fallback_values[feature] = self.target_mean_encoders[feature].mean()\n",
    "        \n",
    "        # 舊版的分箱和順序編碼邏輯\n",
    "        latitude_bins = pd.cut(X_copy['latitude'], bins=10, retbins=True)[1]\n",
    "        self.bin_encoders['latitudeBins'] = latitude_bins\n",
    "        \n",
    "        longitude_bins = pd.cut(X_copy['longitude'], bins=10, retbins=True)[1]\n",
    "        self.bin_encoders['longitudeBins'] = longitude_bins\n",
    "        \n",
    "        energy_rating_order = [['G', 'F', 'E', 'D', 'C', 'B', 'A']]\n",
    "        # 處理訓練集中可能存在的未知評級\n",
    "        present_ratings = X_copy['currentEnergyRating'].unique()\n",
    "        for r in present_ratings:\n",
    "            if r not in energy_rating_order[0]:\n",
    "                energy_rating_order[0].append(r)\n",
    "\n",
    "        self.ordinal_encoders['currentEnergyRating'] = OrdinalEncoder(\n",
    "            categories=energy_rating_order,\n",
    "            handle_unknown='use_encoded_value',\n",
    "            unknown_value=-1\n",
    "        ).fit(X_copy[['currentEnergyRating']])\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        # 應用目標編碼\n",
    "        target_encoding_features = [\n",
    "            'street', 'postcode', 'outcode', 'tenure', 'propertyType',\n",
    "            'street_type', 'outcode_proptype', 'outcode_tenure',\n",
    "        \n",
    "            # vvvvvvvvvvvv 新增需要編碼的地理特徵 vvvvvvvvvvvv\n",
    "            'geo_cluster_medium', 'geo_cluster_fine'\n",
    "        ]\n",
    "        for feature in target_encoding_features:\n",
    "            X_transformed[feature] = X_transformed[feature].map(self.target_mean_encoders[feature])\n",
    "            X_transformed[feature] = X_transformed[feature].fillna(self.fallback_values[feature])\n",
    "        \n",
    "        # 緯度和經度分箱 (使用 labels=False，更穩健)\n",
    "        X_transformed['latitudeBins'] = pd.cut(X_transformed['latitude'], bins=self.bin_encoders['latitudeBins'], include_lowest=True, right=True, labels=False)\n",
    "        X_transformed['longitudeBins'] = pd.cut(X_transformed['longitude'], bins=self.bin_encoders['longitudeBins'], include_lowest=True, right=True, labels=False)\n",
    "        \n",
    "        # 能源評級順序編碼\n",
    "        X_transformed['currentEnergyRating'] = self.ordinal_encoders['currentEnergyRating'].transform(\n",
    "            X_transformed[['currentEnergyRating']]\n",
    "        )\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "class HybridModel(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    混合模型：結合趨勢模型和機器學習模型\n",
    "    - 趨勢模型：處理時間序列特徵\n",
    "    - 機器學習模型：處理殘差和其他特徵\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trend_model, machine_model, trend_cols, machine_cols, all_columns):\n",
    "        self.trend_model = trend_model\n",
    "        self.machine_model = machine_model\n",
    "        self.trend_cols = trend_cols\n",
    "        self.machine_cols = machine_cols\n",
    "        self.all_columns = all_columns\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"訓練混合模型\"\"\"\n",
    "        # 對目標變量進行對數轉換以穩定方差\n",
    "        y_log = np.log1p(y)\n",
    "        \n",
    "        # 確保輸入是 DataFrame 格式\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns=self.all_columns)\n",
    "        \n",
    "        # 分離趨勢特徵和機器學習特徵\n",
    "        trend_features = X[self.trend_cols]\n",
    "        machine_features = X[self.machine_cols]\n",
    "        \n",
    "        # 1. 訓練趨勢模型\n",
    "        self.trend_model.fit(trend_features, y_log)\n",
    "        \n",
    "        # 2. 計算趨勢預測的殘差\n",
    "        trend_predictions = self.trend_model.predict(trend_features)\n",
    "        residual = y_log - trend_predictions\n",
    "        \n",
    "        # 3. 用機器學習模型學習殘差\n",
    "        self.machine_model.fit(machine_features, residual)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"進行預測\"\"\"\n",
    "        # 確保輸入是 DataFrame 格式\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns=self.all_columns)\n",
    "        \n",
    "        # 分離特徵\n",
    "        trend_features = X[self.trend_cols]\n",
    "        machine_features = X[self.machine_cols]\n",
    "        \n",
    "        # 獲取趨勢預測和機器學習預測\n",
    "        trend_predictions = self.trend_model.predict(trend_features)\n",
    "        machine_predictions = self.machine_model.predict(machine_features)\n",
    "        \n",
    "        # 組合預測結果並反向對數轉換\n",
    "        combined_predictions = trend_predictions + machine_predictions\n",
    "        return np.expm1(combined_predictions)\n",
    "\n",
    "\n",
    "def prepare_features(train_data, test_data, time_series_features):\n",
    "    \"\"\"準備特徵集合\"\"\"\n",
    "    print(\"準備特徵集合...\")\n",
    "    \n",
    "    # 時間序列特徵（用於趨勢模型）\n",
    "    trend_features = time_series_features\n",
    "    \n",
    "    # 機器學習特徵（用於殘差模型）\n",
    "    machine_learning_features = [\n",
    "        # --- 原有特徵 ---\n",
    "        'street', 'postcode', 'outcode', 'latitudeBins', 'longitudeBins',\n",
    "        'bathrooms', 'bedrooms', 'rooms', 'floorAreaSqM', 'livingRooms',\n",
    "        'tenure', 'propertyType', 'currentEnergyRating', \n",
    "        'rooms_per_bedroom', 'bath_per_room',\n",
    "        \n",
    "        # --- 之前新增的交叉特徵 ---\n",
    "        'outcode_proptype', 'outcode_tenure',\n",
    "        \n",
    "        # --- 全新的地址特徵 ---\n",
    "        'street_type', 'is_flat_or_apt', 'address_number', 'address_length',\n",
    "        'has_mansion', 'has_penthouse', 'has_cottage', 'has_studio', \n",
    "        'has_garden', 'has_park', 'has_view', 'has_river', 'has_new build',\n",
    "        \n",
    "        # vvvvvvvvvvvv 新增所有地理特徵 vvvvvvvvvvvv\n",
    "        'lat_lon_ratio', 'lat_lon_product',\n",
    "        'dist_to_city_center', 'dist_to_canary_wharf', 'dist_to_westminster', 'dist_to_heathrow',\n",
    "        'geo_cluster_medium', 'geo_cluster_fine'\n",
    "    ]\n",
    "    \n",
    "    # 準備訓練特徵和目標\n",
    "    X_train = train_data.drop('price', axis=1)\n",
    "    y_train = train_data['price']\n",
    "    \n",
    "    # 標準化時間序列特徵\n",
    "    scaler = StandardScaler()\n",
    "    X_train[trend_features] = scaler.fit_transform(X_train[trend_features])\n",
    "    test_data[trend_features] = scaler.transform(test_data[trend_features])\n",
    "    \n",
    "    return X_train, y_train, trend_features, machine_learning_features\n",
    "\n",
    "\n",
    "def create_and_tune_model(X_train, y_train, trend_features, machine_learning_features):\n",
    "    \"\"\"創建並調優模型\"\"\"\n",
    "    print(\"創建混合模型並進行超參數調優...\")\n",
    "    \n",
    "    # 定義模型管道\n",
    "    model_pipeline = {\n",
    "        'HybridModel': Pipeline([\n",
    "            ('Encoder', CustomEncoder()),\n",
    "            ('Model', HybridModel(\n",
    "                trend_model=Ridge(),\n",
    "                machine_model=XGBRegressor(),\n",
    "                trend_cols=trend_features,\n",
    "                machine_cols=machine_learning_features,\n",
    "                all_columns=X_train.columns\n",
    "            ))\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    # 定義超參數搜索空間\n",
    "    hyperparameter_grid = {\n",
    "        'HybridModel': {\n",
    "            'Model__trend_model__alpha': [0.01, 0.1],\n",
    "            'Model__machine_model__n_estimators': [300,600],\n",
    "            'Model__machine_model__max_depth': [4,6,8],\n",
    "            'Model__machine_model__learning_rate': [0.01, 0.005, 0.1],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 進行網格搜索\n",
    "    best_models = {}\n",
    "    for model_name, pipeline in model_pipeline.items():\n",
    "        print(f\"調優 {model_name}...\")\n",
    "        cv = TimeSeriesSplit(n_splits=5)\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline, \n",
    "            hyperparameter_grid[model_name], \n",
    "            cv=cv, \n",
    "            scoring='neg_mean_absolute_error', \n",
    "            n_jobs=-1, \n",
    "            verbose=2, \n",
    "            error_score='raise'\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"{model_name} 最佳參數: {grid_search.best_params_}\")\n",
    "        print(f\"{model_name} 最佳 MAE: {-grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "    \n",
    "    return best_models\n",
    "\n",
    "def create_and_tune_model_with_gridsearch(X_train, y_train, trend_features, machine_learning_features):\n",
    "    \"\"\"\n",
    "    創建混合模型，並使用 GridSearchCV 進行超參數調優。\n",
    "    - 使用 XGBRegressor 作為機器學習模型。\n",
    "    - 使用 TimeSeriesSplit 進行交叉驗證。\n",
    "    \"\"\"\n",
    "    print(\"創建混合模型並使用 GridSearchCV 進行超參數調優...\")\n",
    "\n",
    "    # 1. 定義模型管道，使用 XGBRegressor\n",
    "    # 這裡我們將機器學習模型換成了 XGBRegressor\n",
    "    model_pipeline = Pipeline([\n",
    "        ('Encoder', CustomEncoder()),\n",
    "        ('Model', HybridModel(\n",
    "            trend_model=Ridge(),\n",
    "            machine_model=XGBRegressor(random_state=42, eval_metric='mae'), # 使用 XGBoost\n",
    "            trend_cols=trend_features,\n",
    "            machine_cols=machine_learning_features,\n",
    "            all_columns=X_train.columns\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # 2. 定義與舊版 hybrid-model-trend-ml.py 相似的超參數搜索空間\n",
    "    hyperparameter_grid = {\n",
    "        'Model__trend_model__alpha': [0.01, 0.1],\n",
    "        'Model__machine_model__n_estimators': [500],\n",
    "        'Model__machine_model__max_depth': [9],\n",
    "        'Model__machine_model__learning_rate': [0.01, 0.05, 0.1],\n",
    "    }\n",
    "\n",
    "    # 3. 進行網格搜索，但保留新版中正確的 TimeSeriesSplit\n",
    "    print(\"開始 GridSearchCV 調優...\")\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    grid_search = GridSearchCV(\n",
    "        model_pipeline,\n",
    "        hyperparameter_grid,\n",
    "        cv=tscv,  # <-- 關鍵！我們保留了正確的時間序列交叉驗證方法\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        error_score='raise'\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"GridSearchCV 最佳參數: {grid_search.best_params_}\")\n",
    "    print(f\"GridSearchCV 最佳 MAE (CV): {-grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # 4. 返回訓練好的最佳模型\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def create_ensemble_model(best_models, X_train, y_train):\n",
    "    \"\"\"創建集成模型\"\"\"\n",
    "    print(\"創建集成模型...\")\n",
    "    \n",
    "    # 準備集成模型的估計器列表\n",
    "    ensemble_estimators = [\n",
    "        ('HybridModel', best_models['HybridModel']),\n",
    "    ]\n",
    "    \n",
    "    # 創建投票回歸器\n",
    "    ensemble_model = VotingRegressor(estimators=ensemble_estimators)\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"集成模型: {ensemble_model}\")\n",
    "    \n",
    "    return ensemble_model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train):\n",
    "    \"\"\"評估模型性能\"\"\"\n",
    "    print(\"評估模型性能...\")\n",
    "    \n",
    "    # 預測訓練集\n",
    "    train_predictions = model.predict(X_train)\n",
    "    \n",
    "    # 計算評估指標\n",
    "    mae = mean_absolute_error(y_train, train_predictions)\n",
    "    rmse = mean_squared_error(y_train, train_predictions, squared=False)\n",
    "    r2 = r2_score(y_train, train_predictions)\n",
    "    \n",
    "    print(f\"[訓練集] MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "    \n",
    "    return mae, rmse, r2\n",
    "\n",
    "def clip_outliers(train_df):\n",
    "    print(\"Clip 價格與連續特徵異常值...\")\n",
    "    \n",
    "    # clip price\n",
    "    q1, q3 = train_df['price'].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    train_df['price'] = train_df['price'].clip(upper=upper_bound)\n",
    "    \n",
    "    # Winsorize floorAreaSqM\n",
    "    fq1, fq3 = train_df['floorAreaSqM'].quantile([0.25, 0.75])\n",
    "    fiqr = fq3 - fq1\n",
    "    f_upper = fq3 + 1.5 * fiqr\n",
    "    train_df['floorAreaSqM'] = train_df['floorAreaSqM'].clip(upper=f_upper)\n",
    "    \n",
    "    return train_df\n",
    "def run_optuna_tuning(X_train, y_train, trend_features, machine_features, all_columns, n_trials=10):\n",
    "    \"\"\"使用 Optuna 進行超參數調優\"\"\"\n",
    "    print(\"開始 Optuna 超參數調優...\")\n",
    "    \n",
    "    # 創建 Optuna 研究\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    # 執行優化\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_train, y_train, trend_features, machine_features, all_columns),\n",
    "        n_trials=n_trials,show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    print(f\"最佳參數: {study.best_params}\")\n",
    "    print(f\"最佳 MAE: {study.best_value:.4f}\")\n",
    "    \n",
    "    # 使用最佳參數創建最終模型\n",
    "    best_params = study.best_params\n",
    "    final_model = Pipeline([\n",
    "        ('Encoder', CustomEncoder()),\n",
    "        ('Model', HybridModel(\n",
    "            trend_model=Ridge(alpha=0.1),\n",
    "            machine_model=LGBMRegressor(device='gpu', **best_params),\n",
    "            trend_cols=trend_features,\n",
    "            machine_cols=machine_features,\n",
    "            all_columns=all_columns\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # 訓練最終模型\n",
    "    final_model.fit(X_train, y_train)\n",
    "    \n",
    "    return final_model\n",
    "\n",
    "def train_with_fixed_params(X_train, y_train, trend_features, machine_learning_features, best_params):\n",
    "    \"\"\"使用一組固定的超參數直接訓練最終模型\"\"\"\n",
    "    print(\"使用固定參數進行最終模型訓練...\")\n",
    "    print(f\"使用參數: {best_params}\")\n",
    "\n",
    "    # 創建包含 XGBoost 的模型管道\n",
    "    model_pipeline = Pipeline([\n",
    "        ('Encoder', CustomEncoder()),\n",
    "        ('Model', HybridModel(\n",
    "            trend_model=Ridge(), # alpha 將在下一步被設定\n",
    "            machine_model=XGBRegressor(random_state=42, eval_metric='mae'), # 其他參數將在下一步被設定\n",
    "            trend_cols=trend_features,\n",
    "            machine_cols=machine_learning_features,\n",
    "            all_columns=X_train.columns\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # 使用 set_params() 將您提供的參數應用到管道中\n",
    "    model_pipeline.set_params(**best_params)\n",
    "\n",
    "    # 訓練模型\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    print(\"模型訓練完成。\")\n",
    "    return model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T06:46:10.909807Z",
     "iopub.status.busy": "2025-06-19T06:46:10.909515Z",
     "iopub.status.idle": "2025-06-19T06:46:10.918779Z",
     "shell.execute_reply": "2025-06-19T06:46:10.918060Z",
     "shell.execute_reply.started": "2025-06-19T06:46:10.909774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"載入並準備訓練和測試資料\"\"\"\n",
    "    print(\"載入資料...\")\n",
    "    train_df = pd.read_csv('/kaggle/input/london-house-price/train.csv')\n",
    "    test_df = pd.read_csv('/kaggle/input/london-house-price/test.csv')\n",
    "    \n",
    "    # 為測試集添加空的價格欄位\n",
    "    test_df['price'] = np.nan\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def generate_submission(model, test_data):\n",
    "    \"\"\"生成提交檔案\"\"\"\n",
    "    print(\"生成提交檔案...\")\n",
    "    \n",
    "    # 載入提交模板\n",
    "    submission = pd.read_csv('/kaggle/input/london-house-price/sample_submission.csv')\n",
    "    \n",
    "    # 進行預測\n",
    "    test_features = test_data.drop('price', axis=1)\n",
    "    submission['price'] = model.predict(test_features)\n",
    "    \n",
    "    # 儲存提交檔案\n",
    "    submission.to_csv('submission_TimeSeriesSplit.csv', index=False)\n",
    "    print(\"提交檔案已儲存為 submission_TimeSeriesSplit.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T11:05:53.705608Z",
     "iopub.status.busy": "2025-06-19T11:05:53.704904Z",
     "iopub.status.idle": "2025-06-19T11:06:12.655132Z",
     "shell.execute_reply": "2025-06-19T11:06:12.654276Z",
     "shell.execute_reply.started": "2025-06-19T11:05:53.705587Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 倫敦房價預測 - 混合模型 ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_and_prepare_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# 執行主程序\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== 倫敦房價預測 - 混合模型 ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1. 載入資料\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m train_df, test_df = \u001b[43mload_and_prepare_data\u001b[49m()\n\u001b[32m      7\u001b[39m data_list = [train_df, test_df]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 2. 創建時間特徵\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'load_and_prepare_data' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"主要執行流程（已整合所有特徵工程）\"\"\"\n",
    "    print(\"=== 倫敦房價預測 - 最終整合版 ===\")\n",
    "    \n",
    "    # 1. 載入資料\n",
    "    train_df, test_df = load_and_prepare_data()\n",
    "\n",
    "    # --- 核心特徵工程管線 ---\n",
    "    # 2. 基礎時間特徵\n",
    "    data_list = create_time_features([train_df, test_df])\n",
    "    train_df, test_df = data_list[0], data_list[1]\n",
    "    train_df = train_df.set_index('time')\n",
    "    test_df = test_df.set_index('time')\n",
    "\n",
    "    # 3. 地址文字特徵 (基礎 + 您的高級版本)\n",
    "    data_list = preprocess_address_features([train_df, test_df])\n",
    "    data_list = engineer_address_features(data_list)\n",
    "    train_df, test_df = data_list[0], data_list[1]\n",
    "\n",
    "    # 4. 處理缺失值 (使用您筆記本中的版本)\n",
    "    data_list = handle_missing_values([train_df, test_df])\n",
    "    train_df, test_df = data_list[0], data_list[1]\n",
    "\n",
    "    # 5. 創建時間序列趨勢特徵\n",
    "    train_df, test_df, time_series_features = create_time_series_features(train_df, test_df)\n",
    "\n",
    "    # 6. 創建額外交叉特徵\n",
    "    data_list = create_additional_features([train_df, test_df])\n",
    "    train_df, test_df = data_list[0], data_list[1]\n",
    "    \n",
    "    # 7. 創建地理空間特徵 (最關鍵的新增部分)\n",
    "    # a. 提前準備好目標變量，因為目標編碼需要它\n",
    "    y_train_log = np.log1p(train_df['price'])\n",
    "    # b. 創建一個字典來存儲學習到的模型和映射\n",
    "    geo_config = {} \n",
    "    # c. 執行地理特徵工程\n",
    "    train_df, test_df = engineer_geo_features(\n",
    "        train_df, test_df,y_train_log\n",
    "    )\n",
    "    \n",
    "    # 8. 準備最終特徵 (在所有特徵創建完畢後，只調用一次)\n",
    "    X_train, y_train, trend_features, machine_learning_features = prepare_features(\n",
    "        train_df, test_df, time_series_features\n",
    "    )\n",
    "    \n",
    "    # 9. 使用固定參數訓練模型\n",
    "    best_params = {\n",
    "        'Model__machine_model__learning_rate': 0.01,\n",
    "        'Model__machine_model__max_depth': 9,\n",
    "        'Model__machine_model__n_estimators': 500,\n",
    "        'Model__trend_model__alpha': 0.01\n",
    "    }\n",
    "    final_model = train_with_fixed_params(\n",
    "        X_train, y_train, trend_features, machine_learning_features, best_params\n",
    "    )\n",
    "\n",
    "    # 10. 評估和提交\n",
    "    evaluate_model(final_model, X_train, y_train)\n",
    "    generate_submission(final_model, test_df)\n",
    "    \n",
    "    print(\"=== 程序執行完成 ===\")\n",
    "\n",
    "\n",
    "# 執行主程序\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7687677,
     "sourceId": 12204104,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7693306,
     "sourceId": 12212383,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "dm_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
