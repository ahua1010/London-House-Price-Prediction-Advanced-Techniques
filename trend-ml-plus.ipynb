{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T15:14:53.729341Z",
     "iopub.status.busy": "2025-06-23T15:14:53.728597Z",
     "iopub.status.idle": "2025-06-23T15:14:53.792051Z",
     "shell.execute_reply": "2025-06-23T15:14:53.791478Z",
     "shell.execute_reply.started": "2025-06-23T15:14:53.729318Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "倫敦房價預測 - 混合模型（趨勢分析 + 機器學習）\n",
    "使用時間序列特徵結合機器學習進行房價預測\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier\n",
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold\n",
    "\n",
    "def objective(trial, X_encoded, y, trend_cols, machine_cols_encoded, all_columns_encoded):\n",
    "    \"\"\"\n",
    "    Optuna objective function for pre-encoded, GPU-ready data.\n",
    "    This is the high-performance version.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"device\": \"gpu\",\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 2000, 6000, step=500),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 6, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-2, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-2, 10.0, log=True),\n",
    "        \"random_state\": 42,\n",
    "        \"verbose\": -1\n",
    "    }\n",
    "\n",
    "    # 直接使用 HybridModel，不再需要 Pipeline\n",
    "    model = HybridModel(\n",
    "        trend_model=Ridge(alpha=0.1),\n",
    "        machine_model=LGBMRegressor(**params),\n",
    "        trend_cols=trend_cols,\n",
    "        machine_cols=machine_cols_encoded,\n",
    "        all_columns=all_columns_encoded\n",
    "    )\n",
    "\n",
    "    price_bins = pd.qcut(y, q=10, labels=False, duplicates='drop')\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    scores = []\n",
    "    # 序列化執行迴圈，這是 GPU 最高效的工作方式\n",
    "    for train_idx, val_idx in cv.split(X_encoded, price_bins):\n",
    "        X_train_fold, X_val_fold = X_encoded.iloc[train_idx], X_encoded.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        preds = model.predict(X_val_fold)\n",
    "        fold_score = mean_absolute_error(y_val_fold, preds)\n",
    "        scores.append(fold_score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "def create_time_features(data_list):\n",
    "    \"\"\"創建時間相關特徵\"\"\"\n",
    "    print(\"創建時間特徵...\")\n",
    "    for data in data_list:\n",
    "        # 創建時間索引\n",
    "        data['time'] = pd.to_datetime(dict(\n",
    "            year=data['sale_year'], \n",
    "            month=data['sale_month'], \n",
    "            day=15\n",
    "        ))\n",
    "        data['time'] = data['time'].dt.to_period('M')\n",
    "        \n",
    "        # 創建數值型時間特徵\n",
    "        data['time_numeric'] = (\n",
    "            (data['time'].dt.to_timestamp() - data['time'].min().to_timestamp()) / \n",
    "            np.timedelta64(1, 'D')\n",
    "        )\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "\n",
    "def preprocess_address_features(data_list):\n",
    "    \"\"\"處理地址相關特徵\"\"\"\n",
    "    print(\"處理地址特徵...\")\n",
    "    for data in data_list:\n",
    "        # 提取街道資訊\n",
    "        data['street'] = data['fullAddress'].apply(\n",
    "            lambda address: ' '.join(address.split(',')[-3].split(' ')[-2:])\n",
    "        )\n",
    "        \n",
    "        # 處理郵遞區號\n",
    "        data['postcode'] = data['postcode'].apply(\n",
    "            lambda postcode: postcode.split(' ')[1]\n",
    "        )\n",
    "        \n",
    "        # 移除國家欄位（所有資料都是同一個國家）\n",
    "        data.drop('country', axis=1, inplace=True)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "def engineer_address_features(data_list):\n",
    "    \"\"\"從 fullAddress 中提取更豐富的特徵\"\"\"\n",
    "    print(\"進行高級地址特徵工程...\")\n",
    "    \n",
    "    for df in data_list:\n",
    "        # 將地址轉為小寫以便搜索\n",
    "        address_lower = df['fullAddress'].str.lower()\n",
    "\n",
    "        # 1. 提取街道類型\n",
    "        df['street_type'] = address_lower.str.extract(r'\\b(road|street|avenue|lane|square|drive|court|place|gardens|mews)\\b', expand=False).fillna('unknown')\n",
    "\n",
    "        # 2. 是否為公寓/樓層\n",
    "        df['is_flat_or_apt'] = address_lower.str.contains(r'flat|apartment|unit|floor|level').astype(int)\n",
    "\n",
    "        # 3. 提取數字資訊 (可能代表門牌號或公寓號)\n",
    "        # 提取第一個出現的數字序列\n",
    "        df['address_number'] = address_lower.str.extract(r'(\\d+)').astype(float).fillna(0)\n",
    "\n",
    "        # 4. 地址長度 (一個代理特徵，更長的地址可能意味著更複雜的建築)\n",
    "        df['address_length'] = df['fullAddress'].str.len()\n",
    "\n",
    "        # 5. 關鍵詞計數\n",
    "        keywords = ['mansion', 'penthouse', 'cottage', 'studio', 'garden', 'park', 'view', 'river', 'new build']\n",
    "        for keyword in keywords:\n",
    "            df[f'has_{keyword}'] = address_lower.str.contains(keyword).astype(int)\n",
    "            \n",
    "    return data_list\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    計算兩點之間的 haversine 距離（單位：公里）\n",
    "    \"\"\"\n",
    "    R = 6371  # 地球半徑 (km)\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    return R * 2 * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def engineer_geo_features(train_df, test_df):\n",
    "    \"\"\"\n",
    "    執行地理特徵工程（已移除所有價格相關特徵，只保留純幾何特徵）。\n",
    "    \"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    print(\"-> 開始執行地理特徵工程（僅純幾何特徵）...\")\n",
    "    \n",
    "    train_df_copy = train_df.copy()\n",
    "    test_df_copy = test_df.copy()\n",
    "    \n",
    "    for df in [train_df_copy, test_df_copy]:\n",
    "        # === 1. 基本地理數學特徵 ===\n",
    "        df['lat_lon_ratio'] = df['latitude'] / (df['longitude'] + 1e-9)\n",
    "        df['lat_lon_product'] = df['latitude'] * df['longitude']\n",
    "\n",
    "        # === 2. 倫敦重要地標距離特徵 ===\n",
    "        london_landmarks = {\n",
    "            'city_center': (51.5074, -0.1278), 'canary_wharf': (51.5055, -0.0195),\n",
    "            'westminster': (51.4994, -0.1244), 'heathrow': (51.4700, -0.4543)\n",
    "        }\n",
    "        for name, (lat, lon) in london_landmarks.items():\n",
    "            df[f'dist_to_{name}'] = haversine_distance(df['latitude'], df['longitude'], lat, lon)\n",
    "\n",
    "    # === 3. 地理聚類特徵 (Fit on train, transform both) ===\n",
    "    # 這一步是安全的，因為它只基於座標，不涉及價格\n",
    "    coords_train = train_df_copy[['latitude', 'longitude']].values\n",
    "    coords_test = test_df_copy[['latitude', 'longitude']].values\n",
    "    cluster_configs = {'geo_cluster_medium': 20, 'geo_cluster_fine': 50}\n",
    "    \n",
    "    for name, k in cluster_configs.items():\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        train_df_copy[name] = kmeans.fit_predict(coords_train)\n",
    "        test_df_copy[name] = kmeans.predict(coords_test)\n",
    "        \n",
    "    print(\"-> 純地理特徵工程完成。\")\n",
    "    return train_df_copy, test_df_copy\n",
    "\n",
    "def impute_missing_values_with_strategy(data_list, column_name, strategy='most_frequent'):\n",
    "    \"\"\"使用指定策略填補缺失值\"\"\"\n",
    "    print(f\"填補 {column_name} 的缺失值（策略：{strategy}）...\")\n",
    "    \n",
    "    # 從訓練資料學習填補策略\n",
    "    train_data = data_list[0]  # 第一個是訓練資料\n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    imputer.fit(train_data[[column_name]])\n",
    "    \n",
    "    # 對所有資料集應用填補\n",
    "    for data in data_list:\n",
    "        data[column_name] = imputer.transform(data[[column_name]]).ravel()\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "\n",
    "def impute_with_regression(data_list, target_column, feature_column):\n",
    "    \"\"\"使用回歸模型填補缺失值\"\"\"\n",
    "    print(f\"使用 {feature_column} 預測填補 {target_column} 的缺失值...\")\n",
    "    \n",
    "    train_data = data_list[0]\n",
    "    test_data = data_list[1]\n",
    "    \n",
    "    # 準備完整的訓練資料\n",
    "    complete_train_data = train_data.dropna(subset=[target_column, feature_column])\n",
    "    X_train = complete_train_data[[feature_column]]\n",
    "    y_train = complete_train_data[target_column]\n",
    "    \n",
    "    # 訓練回歸模型\n",
    "    regression_model = Ridge()\n",
    "    regression_model.fit(X_train, y_train)\n",
    "    \n",
    "    # 填補訓練集的缺失值\n",
    "    missing_train_mask = train_data[target_column].isna()\n",
    "    if missing_train_mask.any():\n",
    "        missing_train_features = train_data.loc[missing_train_mask, [feature_column]]\n",
    "        train_data.loc[missing_train_mask, target_column] = regression_model.predict(missing_train_features)\n",
    "    \n",
    "    # 填補測試集的缺失值\n",
    "    missing_test_mask = test_data[target_column].isna()\n",
    "    if missing_test_mask.any():\n",
    "        missing_test_features = test_data.loc[missing_test_mask, [feature_column]]\n",
    "        test_data.loc[missing_test_mask, target_column] = regression_model.predict(missing_test_features)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "def handle_missing_values(data_list):\n",
    "    \"\"\"處理所有缺失值\"\"\"\n",
    "    print(\"開始處理缺失值...\")\n",
    "    \n",
    "    # 使用最頻繁值填補面積\n",
    "    data_list = impute_missing_values_with_strategy(data_list, 'floorAreaSqM')\n",
    "    \n",
    "    # 使用面積預測浴室數量\n",
    "    data_list = impute_with_regression(data_list, 'bathrooms', 'floorAreaSqM')\n",
    "    \n",
    "    # 使用面積預測臥室數量\n",
    "    data_list = impute_with_regression(data_list, 'bedrooms', 'floorAreaSqM')\n",
    "    \n",
    "    # 使用最頻繁值填補其他類別特徵\n",
    "    categorical_columns = ['livingRooms', 'tenure', 'propertyType', 'currentEnergyRating']\n",
    "    for column in categorical_columns:\n",
    "        data_list = impute_missing_values_with_strategy(data_list, column)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "\n",
    "def create_time_series_features(train_data, test_data):\n",
    "    \"\"\"創建時間序列特徵\"\"\"\n",
    "    print(\"創建時間序列特徵...\")\n",
    "    \n",
    "    # 創建確定性過程（趨勢、季節性、週期性）\n",
    "    deterministic_process = DeterministicProcess(\n",
    "        index=train_data.index.unique(),\n",
    "        constant=True,        # 常數項\n",
    "        seasonal=True,        # 季節性\n",
    "        order=12,            # 趨勢階數\n",
    "        drop=True,           # 移除共線性\n",
    "        additional_terms=[CalendarFourier(freq=\"QE\", order=4)],  # 季度傅立葉項\n",
    "    )\n",
    "    \n",
    "    # 為訓練資料添加時間序列特徵\n",
    "    time_features_train = deterministic_process.in_sample()\n",
    "    train_data = train_data.join(time_features_train, how='left')\n",
    "    \n",
    "    # 計算預測相關參數\n",
    "    forecast_origin = train_data.index.max()\n",
    "    forecast_lead = test_data.index.min() - forecast_origin\n",
    "    forecast_horizon = test_data.index.max() - test_data.index.min()\n",
    "    \n",
    "    print(f\"預測起點: {forecast_origin}\")\n",
    "    print(f\"領先時間: {forecast_lead.n} 個月\")\n",
    "    print(f\"預測範圍: {forecast_horizon.n} 個月\")\n",
    "    \n",
    "    # 為測試資料添加時間序列特徵\n",
    "    time_features_test = deterministic_process.out_of_sample(\n",
    "        steps=forecast_horizon.n + forecast_lead.n\n",
    "    )\n",
    "    test_data = test_data.join(time_features_test, how='left')\n",
    "    test_data.index.name = 'time'\n",
    "    \n",
    "    return train_data, test_data, time_features_train.columns.tolist()\n",
    "\n",
    "\n",
    "def create_additional_features(data_list):\n",
    "    \"\"\"創建額外的特徵\"\"\"\n",
    "    print(\"創建額外特徵...\")\n",
    "    \n",
    "    for data in data_list:\n",
    "        # 總房間數 = 臥室 + 起居室\n",
    "        data['rooms'] = data['bedrooms'] + data['livingRooms']\n",
    "        # 總房間數 = 臥室 + 起居室\n",
    "        data['rooms'] = data['bedrooms'] + data['livingRooms']\n",
    "        \n",
    "        # 衍生密度特徵\n",
    "        data['rooms_per_bedroom'] = data['rooms'] / np.maximum(data['bedrooms'], 1)\n",
    "        data['bath_per_room'] = data['bathrooms'] / np.maximum(data['rooms'], 1)\n",
    "\n",
    "        # === 新增：創建交叉特徵 ===\n",
    "        # 將 outcode 和 propertyType 結合，形成更具體的特徵\n",
    "        # 例如 \"SW1_Flat\" (SW1區的公寓)\n",
    "        data['outcode_proptype'] = data['outcode'].astype(str) + \"_\" + data['propertyType'].astype(str)\n",
    "\n",
    "        # 將 outcode 和 tenure 結合\n",
    "        data['outcode_tenure'] = data['outcode'].astype(str) + \"_\" + data['tenure'].astype(str)\n",
    "            \n",
    "    \n",
    "    return data_list\n",
    "\n",
    "class CustomEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    智能編碼器，自動處理所有類別特徵，並對特定特徵進行專門處理。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.target_mean_encoders = {}\n",
    "        self.fallback_values = {}\n",
    "        self.bin_encoders = {}\n",
    "        self.ordinal_encoders = {}\n",
    "        self.object_cols_to_encode = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['price'] = y\n",
    "        \n",
    "        # 1. 智能識別所有需要進行目標編碼的 object 欄位\n",
    "        #    排除 'currentEnergyRating'，因其需要特殊的順序編碼\n",
    "        self.object_cols_to_encode = [\n",
    "            col for col in X_copy.select_dtypes(include='object').columns\n",
    "            if col != 'currentEnergyRating'\n",
    "        ]\n",
    "        \n",
    "        for feature in self.object_cols_to_encode:\n",
    "            self.target_mean_encoders[feature] = X_copy.groupby(feature)['price'].mean()\n",
    "            self.fallback_values[feature] = self.target_mean_encoders[feature].mean()\n",
    "        \n",
    "        # 2. 對緯度和經度進行分箱編碼 (保持不變)\n",
    "        latitude_bins = pd.cut(X_copy['latitude'], bins=10, retbins=True)[1]\n",
    "        self.bin_encoders['latitudeBins'] = latitude_bins\n",
    "        longitude_bins = pd.cut(X_copy['longitude'], bins=10, retbins=True)[1]\n",
    "        self.bin_encoders['longitudeBins'] = longitude_bins\n",
    "        \n",
    "        # 3. 對能源評級進行特殊的順序編碼 (保持不變)\n",
    "        energy_rating_order = [['G', 'F', 'E', 'D', 'C', 'B', 'A']]\n",
    "        present_ratings = X_copy['currentEnergyRating'].unique()\n",
    "        for r in present_ratings:\n",
    "            if r not in energy_rating_order[0]:\n",
    "                energy_rating_order[0].append(r)\n",
    "        self.ordinal_encoders['currentEnergyRating'] = OrdinalEncoder(\n",
    "            categories=energy_rating_order,\n",
    "            handle_unknown='use_encoded_value',\n",
    "            unknown_value=-1\n",
    "        ).fit(X_copy[['currentEnergyRating']])\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        # 1. 應用目標編碼\n",
    "        for feature in self.object_cols_to_encode:\n",
    "            if feature in X_transformed.columns:\n",
    "                X_transformed[feature] = X_transformed[feature].map(self.target_mean_encoders[feature])\n",
    "                X_transformed[feature] = X_transformed[feature].fillna(self.fallback_values[feature])\n",
    "        \n",
    "        # 2. 應用分箱編碼\n",
    "        X_transformed['latitudeBins'] = pd.cut(X_transformed['latitude'], bins=self.bin_encoders['latitudeBins'], include_lowest=True, right=True, labels=False)\n",
    "        X_transformed['longitudeBins'] = pd.cut(X_transformed['longitude'], bins=self.bin_encoders['longitudeBins'], include_lowest=True, right=True, labels=False)\n",
    "        \n",
    "        # 3. 應用順序編碼\n",
    "        X_transformed['currentEnergyRating'] = self.ordinal_encoders['currentEnergyRating'].transform(\n",
    "            X_transformed[['currentEnergyRating']]\n",
    "        )\n",
    "        \n",
    "        return X_transformed\n",
    "    \n",
    "class HybridModel(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    混合模型：結合趨勢模型(CPU)和機器學習模型(GPU)\n",
    "    - 趨勢模型：在 CPU 上處理時間序列特徵\n",
    "    - 機器學習模型：在 GPU 上處理殘差和其他特徵\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trend_model, machine_model, trend_cols, machine_cols, all_columns):\n",
    "        self.trend_model = trend_model\n",
    "        self.machine_model = machine_model\n",
    "        self.trend_cols = trend_cols\n",
    "        self.machine_cols = machine_cols\n",
    "        self.all_columns = all_columns\n",
    "        self.machine_cols_encoded_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"訓練混合模型\"\"\"\n",
    "        # 管道已對 X 進行編碼，X 是 DataFrame\n",
    "        y_log = np.log1p(y)\n",
    "        \n",
    "        trend_features = X[self.trend_cols]\n",
    "        \n",
    "        # 機器學習特徵是除了趨勢特徵之外的所有特徵\n",
    "        machine_cols_encoded = [col for col in X.columns if col not in self.trend_cols]\n",
    "        self.machine_cols_encoded_ = machine_cols_encoded # 儲存以供 predict 使用\n",
    "        machine_features = X[machine_cols_encoded]\n",
    "        \n",
    "        # 1. 訓練趨勢模型 (在 CPU 上)\n",
    "        self.trend_model.fit(trend_features, y_log)\n",
    "        \n",
    "        # 2. 計算殘差 (在 CPU 上)\n",
    "        trend_predictions = self.trend_model.predict(trend_features)\n",
    "        residual = y_log - trend_predictions\n",
    "        \n",
    "        # 3. 用機器學習模型學習殘差 (LGBM 在 GPU 上)\n",
    "        self.machine_model.fit(machine_features, residual)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    # ...緊接著 fit 方法的結尾...\n",
    "\n",
    "    def predict_components(self, X):\n",
    "        \"\"\"預測趨勢和殘差的組成部分（在對數空間中）\"\"\"\n",
    "        # 管道已對 X 進行編碼，X 是 DataFrame\n",
    "        trend_features = X[self.trend_cols]\n",
    "        machine_features = X[self.machine_cols_encoded_]\n",
    "        \n",
    "        # 趨勢預測(CPU)和機器學習預測(GPU)\n",
    "        trend_predictions = self.trend_model.predict(trend_features)\n",
    "        machine_predictions = self.machine_model.predict(machine_features)\n",
    "        \n",
    "        return trend_predictions, machine_predictions\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"進行預測\"\"\"\n",
    "        trend_predictions, machine_predictions = self.predict_components(X)\n",
    "        \n",
    "        # 組合預測結果\n",
    "        combined_predictions = trend_predictions + machine_predictions\n",
    "        return np.expm1(combined_predictions)\n",
    "\n",
    "def better_features(features, target_series, is_train, config, columns_to_combine=None, quick_test=False):\n",
    "    \"\"\"\n",
    "    自動化生成和篩選算術組合特徵。\n",
    "    - 訓練模式 (is_train=True): 探索新特徵，評估其與目標的相關性及與現有特徵的冗餘性，\n",
    "      並將有效的特徵組合儲存到 config 中。\n",
    "    - 推論模式 (is_train=False): 從 config 中讀取已儲存的特徵組合，並應用到數據集上。\n",
    "    \"\"\"\n",
    "    print(\"-> 執行特徵組合...\")\n",
    "\n",
    "    if quick_test:\n",
    "        print(\"    啟用快速測試模式，僅使用少量核心特徵進行組合。\")\n",
    "        core_features_for_combination = ['floorAreaSqM', 'total_rooms', 'lat_lon_ratio']\n",
    "        features_to_use = [f for f in core_features_for_combination if f in features.columns]\n",
    "    else:\n",
    "        # 確保只選擇數值型特徵\n",
    "        features_to_use = features.select_dtypes(include=np.number).columns.tolist()\n",
    "        # 移除 'index' 列（如果存在）\n",
    "        if 'index' in features_to_use:\n",
    "            features_to_use.remove('index')\n",
    "\n",
    "    if len(features_to_use) < 2:\n",
    "        print(\"    可用於組合的數值特徵少於2個，跳過此步驟。\")\n",
    "        if is_train:\n",
    "            config['better_features_list'] = [] # 確保鍵存在\n",
    "        return features, config\n",
    "\n",
    "    if is_train:\n",
    "        if target_series is None:\n",
    "            print(\"    在訓練模式下未提供目標變數，跳過特徵組合。\")\n",
    "            config['better_features_list'] = [] # 確保鍵存在\n",
    "            return features, config\n",
    "\n",
    "        # --- 使用 NumPy 進行向量化計算以大幅提高性能 ---\n",
    "        print(\"    初始化 NumPy 矩陣以加速相關性計算...\")\n",
    "        \n",
    "        target_vector = target_series.values\n",
    "        target_vector_norm = (target_vector - np.mean(target_vector)) / np.std(target_vector)\n",
    "\n",
    "        existing_features_matrix = features[features_to_use].values\n",
    "        \n",
    "        mean_existing = np.mean(existing_features_matrix, axis=0)\n",
    "        std_existing = np.std(existing_features_matrix, axis=0)\n",
    "        std_existing[std_existing == 0] = 1\n",
    "        normalized_existing_matrix = (existing_features_matrix - mean_existing) / std_existing\n",
    "        \n",
    "        n_rows = len(target_vector)\n",
    "        new_combinations_list = [] \n",
    "        new_features_to_add = []\n",
    "\n",
    "        print(\"    正在搜索最佳算術組合 (使用向量化)...\")\n",
    "        \n",
    "        for i in tqdm(range(len(features_to_use)), desc=\"    組合特徵搜索\", leave=False):\n",
    "            for j in range(i, len(features_to_use)):\n",
    "                col1 = features_to_use[i]\n",
    "                col2 = features_to_use[j]\n",
    "\n",
    "                for op in ['*', '/', '-', '+']:\n",
    "                    if col1 == col2 and op in ['-']: continue\n",
    "                    if i > j and op in ['+', '*']: continue\n",
    "\n",
    "                    new_feature_name = f'{col1}_{op}_{col2}'\n",
    "                    \n",
    "                    if op == '+': new_feature_series = features[col1] + features[col2]\n",
    "                    elif op == '-': new_feature_series = features[col1] - features[col2]\n",
    "                    elif op == '*': new_feature_series = features[col1] * features[col2]\n",
    "                    elif op == '/': new_feature_series = features[col1] / (features[col2] + 1e-9)\n",
    "                    \n",
    "                    if new_feature_series.isnull().any() or np.isinf(new_feature_series).any():\n",
    "                        continue\n",
    "\n",
    "                    new_feature_vector = new_feature_series.values\n",
    "                    \n",
    "                    mean_new = np.mean(new_feature_vector)\n",
    "                    std_new = np.std(new_feature_vector)\n",
    "                    if std_new < 1e-9: continue\n",
    "                    \n",
    "                    new_feature_norm = (new_feature_vector - mean_new) / std_new\n",
    "                    correlation_with_target = np.dot(target_vector_norm, new_feature_norm) / n_rows\n",
    "                    \n",
    "                    if abs(correlation_with_target) < 0.05:\n",
    "                        continue\n",
    "\n",
    "                    correlations_with_existing = np.dot(new_feature_norm.T, normalized_existing_matrix) / n_rows\n",
    "                    if np.max(np.abs(correlations_with_existing)) > 0.95:\n",
    "                        continue\n",
    "                    \n",
    "                    # 將 new_feature_name 賦給 Series，以便後續 concat\n",
    "                    new_feature_series.name = new_feature_name \n",
    "                    new_features_to_add.append(new_feature_series) # <--- 不直接賦值，而是加入 list\n",
    "                    \n",
    "                    \n",
    "                    # features[new_feature_name] = new_feature_series\n",
    "                    normalized_existing_matrix = np.c_[normalized_existing_matrix, new_feature_norm]\n",
    "                    new_combinations_list.append((col1, col2, op))\n",
    "        \n",
    "        # 在所有迴圈結束後，一次性合併所有新特徵\n",
    "        if new_features_to_add:\n",
    "            features = pd.concat([features] + new_features_to_add, axis=1)\n",
    "        \n",
    "        print(f\"    發現 {len(new_combinations_list)} 個新組合特徵。\")\n",
    "        config['better_features_list'] = new_combinations_list\n",
    "        return features, config\n",
    "\n",
    "    else: # is_train == False\n",
    "        if 'better_features_list' not in config or not config['better_features_list']:\n",
    "            print(\"    在非訓練模式下，未找到已儲存的特徵組合，跳過此步驟。\")\n",
    "            return features, config\n",
    "        \n",
    "        combinations = config['better_features_list']\n",
    "        new_cols_dict = {}\n",
    "        print(f\"    應用 {len(combinations)} 個已儲存的特徵組合。\")\n",
    "        for col1, col2, op in combinations:\n",
    "            new_col_name = f'{col1}_{op}_{col2}'\n",
    "            if op == '+': new_cols_dict[new_col_name] = features[col1] + features[col2]\n",
    "            elif op == '-': new_cols_dict[new_col_name] = features[col1] - features[col2]\n",
    "            elif op == '*': new_cols_dict[new_col_name] = features[col1] * features[col2]\n",
    "            elif op == '/': new_cols_dict[new_col_name] = features[col1] / (features[col2] + 1e-9)\n",
    "        \n",
    "        # 一次性從字典創建 DataFrame 並合併\n",
    "        if new_cols_dict:\n",
    "            new_features_df = pd.DataFrame(new_cols_dict, index=features.index)\n",
    "            features = pd.concat([features, new_features_df], axis=1)\n",
    "\n",
    "        return features.copy(), config\n",
    "\n",
    "# 一個自定義的元模型，它的唯一工作就是將輸入的預測值相加\n",
    "class SummingRegressor(BaseEstimator, RegressorMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        # 不需要學習任何東西\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # X 是一個包含兩欄的 numpy array (趨勢預測, 殘差預測)\n",
    "        # 我們沿著欄位（axis=1）將它們相加\n",
    "        return np.sum(X, axis=1)\n",
    "\n",
    "\n",
    "class StackingModel(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Stacking 集成模型:\n",
    "    \"\"\"\n",
    "    def __init__(self, base_models, meta_model, n_splits=5):\n",
    "        self.base_models = base_models  # 注意這裡從 base_model 改為 base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_splits = n_splits\n",
    "        self.trained_models_ = [] # 存儲每個 fold 訓練好的所有基礎模型\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        print(f\"\\n=== 開始 Stacking 訓練 ({len(self.base_models)} 種基礎模型) ===\")\n",
    "        \n",
    "        cv = KFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        # 建立一個列表，用來存放每個 fold 的模型列表\n",
    "        self.trained_models_ = [[] for _ in range(self.n_splits)]\n",
    "        \n",
    "        # 我們仍然需要 OOF 預測來評估，但現在維度變了\n",
    "        # 每一種基礎模型都會產生一組 OOF 預測\n",
    "        oof_predictions_list = [np.zeros_like(y, dtype=float) for _ in self.base_models]\n",
    "\n",
    "        for i, (train_idx, val_idx) in enumerate(tqdm(cv.split(X, y), total=self.n_splits, desc=\"    訓練 Fold 模型\")):\n",
    "            X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train_fold = y.iloc[train_idx]\n",
    "            \n",
    "            # 對於每一種基礎模型，都進行訓練和預測\n",
    "            for model_idx, base_model in enumerate(self.base_models):\n",
    "                fold_model = clone(base_model)\n",
    "                fold_model.fit(X_train_fold, y_train_fold)\n",
    "                self.trained_models_[i].append(fold_model) # 存儲訓練好的模型\n",
    "                \n",
    "                # 在驗證集上產生預測 (注意：fold_model.predict 直接返回價格)\n",
    "                val_preds = fold_model.predict(X_val_fold)\n",
    "                oof_predictions_list[model_idx][val_idx] = val_preds\n",
    "        \n",
    "        print(\"    OOF 特徵生成完畢。\")\n",
    "        \n",
    "        # 評估簡單平均模型的性能\n",
    "        print(\"\\n    --- 評估 OOF 上的簡單平均模型 ---\")\n",
    "        # 將所有模型的 OOF 預測取平均\n",
    "        final_oof_preds = np.mean(oof_predictions_list, axis=0)\n",
    "        simple_avg_mae = mean_absolute_error(y, final_oof_preds)\n",
    "        print(f\"    -> OOF 簡單平均 MAE: {simple_avg_mae:.4f}\")\n",
    "        \n",
    "        print(\"=== Stacking 訓練完成。 ===\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"    Ensemble 預測 (Averaging)...\")\n",
    "        \n",
    "        predictions_list = []\n",
    "        # 遍歷每一個 Fold\n",
    "        for fold_models in self.trained_models_:\n",
    "            # 在該 Fold 中，遍歷所有類型的基礎模型\n",
    "            for model in fold_models:\n",
    "                predictions_list.append(model.predict(X))\n",
    "        \n",
    "        # 對所有預測（n_splits * n_base_models 個）取平均\n",
    "        final_predictions = np.mean(predictions_list, axis=0)\n",
    "        \n",
    "        return final_predictions\n",
    "    \n",
    "\n",
    "def prepare_features(train_data, test_data, time_series_features):\n",
    "    \"\"\"\n",
    "    準備特徵集合的最終、最穩健版本。\n",
    "    \"\"\"\n",
    "    print(\"準備特徵集合...\")\n",
    "\n",
    "    # --- 修正 NameError ---\n",
    "    # 在使用前，先定義 trend_features\n",
    "    trend_features = time_series_features\n",
    "    \n",
    "    # 在最開始就丟棄高噪聲的原始文字欄位\n",
    "    raw_text_cols_to_drop = ['fullAddress']\n",
    "    train_data = train_data.drop(columns=[col for col in raw_text_cols_to_drop if col in train_data.columns])\n",
    "    test_data = test_data.drop(columns=[col for col in raw_text_cols_to_drop if col in test_data.columns])\n",
    "    \n",
    "    # 準備訓練特徵和目標\n",
    "    X_train = train_data.drop('price', axis=1)\n",
    "    y_train = train_data['price']\n",
    "\n",
    "    # 機器學習特徵 *名稱列表*\n",
    "    all_cols = X_train.columns\n",
    "    machine_learning_features = [\n",
    "        col for col in all_cols if col not in trend_features\n",
    "    ]\n",
    "    print(f\"定義了 {len(machine_learning_features)} 個機器學習特徵。\")\n",
    "\n",
    "    # 標準化時間序列特徵\n",
    "    scaler = StandardScaler()\n",
    "    features_to_scale = [f for f in trend_features if f in X_train.columns and f in test_data.columns]\n",
    "    \n",
    "    if features_to_scale:\n",
    "        X_train.loc[:, features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "        test_data.loc[:, features_to_scale] = scaler.transform(test_data[features_to_scale])\n",
    "    \n",
    "    # 返回五個值，包括更新後的 test_data\n",
    "    return X_train, y_train, trend_features, machine_learning_features, test_data.copy()\n",
    "\n",
    "\n",
    "def create_and_tune_model(X_train, y_train, trend_features, machine_learning_features):\n",
    "    \"\"\"創建並調優模型\"\"\"\n",
    "    print(\"創建混合模型並進行超參數調優...\")\n",
    "    \n",
    "    # 定義模型管道\n",
    "    model_pipeline = {\n",
    "        'HybridModel': Pipeline([\n",
    "            ('Encoder', CustomEncoder()),\n",
    "            ('Model', HybridModel(\n",
    "                trend_model=Ridge(),\n",
    "                machine_model=LGBMRegressor(device='gpu', verbose=-1),\n",
    "                trend_cols=trend_features,\n",
    "                machine_cols=machine_learning_features,\n",
    "                all_columns=X_train.columns\n",
    "            ))\n",
    "        ]),\n",
    "    }\n",
    "    \n",
    "    # 定義超參數搜索空間\n",
    "    hyperparameter_grid = {\n",
    "        'HybridModel': {\n",
    "            'Model__trend_model__alpha': [0.01, 0.1],\n",
    "            'Model__machine_model__n_estimators': [300,600],\n",
    "            'Model__machine_model__max_depth': [4,6,8],\n",
    "            'Model__machine_model__learning_rate': [0.01, 0.005, 0.1],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 進行網格搜索\n",
    "    best_models = {}\n",
    "    for model_name, pipeline in model_pipeline.items():\n",
    "        print(f\"調優 {model_name}...\")\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline, \n",
    "            hyperparameter_grid[model_name], \n",
    "            cv=cv, \n",
    "            scoring='neg_mean_absolute_error', \n",
    "            n_jobs=-1, \n",
    "            verbose=2, \n",
    "            error_score='raise'\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"{model_name} 最佳參數: {grid_search.best_params_}\")\n",
    "        print(f\"{model_name} 最佳 MAE: {-grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "    \n",
    "    return best_models\n",
    "\n",
    "def create_ensemble_model(best_models, X_train, y_train):\n",
    "    \"\"\"創建集成模型\"\"\"\n",
    "    print(\"創建集成模型...\")\n",
    "    \n",
    "    # 準備集成模型的估計器列表\n",
    "    ensemble_estimators = [\n",
    "        ('HybridModel', best_models['HybridModel']),\n",
    "    ]\n",
    "    \n",
    "    # 創建投票回歸器\n",
    "    ensemble_model = VotingRegressor(estimators=ensemble_estimators)\n",
    "    ensemble_model.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"集成模型: {ensemble_model}\")\n",
    "    \n",
    "    return ensemble_model\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train):\n",
    "    \"\"\"評估模型性能\"\"\"\n",
    "    print(\"評估模型性能...\")\n",
    "    \n",
    "    # 預測訓練集\n",
    "    train_predictions = model.predict(X_train)\n",
    "    \n",
    "    # 計算評估指標\n",
    "    mae = mean_absolute_error(y_train, train_predictions)\n",
    "    rmse = mean_squared_error(y_train, train_predictions, squared=False)\n",
    "    r2 = r2_score(y_train, train_predictions)\n",
    "    \n",
    "    print(f\"[訓練集] MAE: {mae:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "    \n",
    "    return mae, rmse, r2\n",
    "\n",
    "\n",
    "def run_optuna_tuning(X_train_encoded, y_train, trend_features, machine_learning_features_encoded, all_columns_encoded, n_trials=100):\n",
    "    \"\"\"使用 Optuna 進行超參數調優（使用預處理數據）\"\"\"\n",
    "    print(\"開始 Optuna 超參數調優 (使用預處理數據)...\")\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_train_encoded, y_train, trend_features, machine_learning_features_encoded, all_columns_encoded),\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    print(f\"最佳參數: {study.best_params}\")\n",
    "    print(f\"最佳 MAE (CV): {study.best_value:.4f}\")\n",
    "    \n",
    "    # 使用最佳參數創建並訓練最終模型\n",
    "    best_params = study.best_params\n",
    "    # 確保參數字典不包含 Optuna 內部使用的 'n_estimators' 等鍵，如果它們不適用於最終模型\n",
    "    final_model = HybridModel(\n",
    "        trend_model=Ridge(alpha=0.1),\n",
    "        machine_model=LGBMRegressor(device='gpu', **best_params),\n",
    "        trend_cols=trend_features,\n",
    "        machine_cols=machine_learning_features_encoded,\n",
    "        all_columns=all_columns_encoded\n",
    "    )\n",
    "    \n",
    "    final_model.fit(X_train_encoded, y_train)\n",
    "    \n",
    "    return final_model\n",
    "\n",
    "def run_optuna_tuning(X_train_encoded, y_train, trend_features, machine_learning_features_encoded, all_columns_encoded, n_trials=100):\n",
    "    \"\"\"使用 Optuna 進行超參數調優（使用預處理數據）\"\"\"\n",
    "    print(\"開始 Optuna 超參數調優 (使用預處理數據)...\")\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_train_encoded, y_train, trend_features, machine_learning_features_encoded, all_columns_encoded),\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    print(f\"最佳參數: {study.best_params}\")\n",
    "    print(f\"最佳 MAE (CV): {study.best_value:.4f}\")\n",
    "    \n",
    "    # 使用最佳參數創建並訓練最終模型\n",
    "    best_params = study.best_params\n",
    "    # 確保參數字典不包含 Optuna 內部使用的 'n_estimators' 等鍵，如果它們不適用於最終模型\n",
    "    final_model = HybridModel(\n",
    "        trend_model=Ridge(alpha=0.1),\n",
    "        machine_model=LGBMRegressor(device='gpu', **best_params),\n",
    "        trend_cols=trend_features,\n",
    "        machine_cols=machine_learning_features_encoded,\n",
    "        all_columns=all_columns_encoded\n",
    "    )\n",
    "    \n",
    "    final_model.fit(X_train_encoded, y_train)\n",
    "    \n",
    "    return final_model\n",
    "\n",
    "def train_with_fixed_params(X_train, y_train, trend_features, machine_learning_features, best_params):\n",
    "    \"\"\"使用一組固定的超參數直接訓練最終模型\"\"\"\n",
    "    print(\"使用固定參數進行最終模型訓練...\")\n",
    "    print(f\"使用參數: {best_params}\")\n",
    "\n",
    "    # 創建包含 XGBoost 的模型管道\n",
    "    model_pipeline = Pipeline([\n",
    "        ('Encoder', CustomEncoder()),\n",
    "        ('Model', HybridModel(\n",
    "            trend_model=Ridge(), # alpha 將在下一步被設定\n",
    "            machine_model=LGBMRegressor(device='gpu', random_state=42), # 其他參數將在下一步被設定\n",
    "            trend_cols=trend_features,\n",
    "            machine_cols=machine_learning_features,\n",
    "            all_columns=X_train.columns\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # 使用 set_params() 將您提供的參數應用到管道中\n",
    "    model_pipeline.set_params(**best_params)\n",
    "\n",
    "    # 訓練模型\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    print(\"模型訓練完成。\")\n",
    "    return model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T15:14:53.793301Z",
     "iopub.status.busy": "2025-06-23T15:14:53.793072Z",
     "iopub.status.idle": "2025-06-23T15:14:53.813048Z",
     "shell.execute_reply": "2025-06-23T15:14:53.812457Z",
     "shell.execute_reply.started": "2025-06-23T15:14:53.793283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"載入並準備訓練和測試資料\"\"\"\n",
    "    print(\"載入資料...\")\n",
    "    train_df = pd.read_csv('/kaggle/input/new-london-house-price/train.csv')\n",
    "    test_df = pd.read_csv('/kaggle/input/new-london-house-price/test.csv')\n",
    "    \n",
    "    # 為測試集添加空的價格欄位\n",
    "    test_df['price'] = np.nan\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "def generate_submission(model, test_data):\n",
    "    \"\"\"生成提交檔案\"\"\"\n",
    "    print(\"生成提交檔案...\")\n",
    "    \n",
    "    # 載入提交模板\n",
    "    submission = pd.read_csv('/kaggle/input/new-london-house-price/sample_submission.csv')\n",
    "    \n",
    "    # 進行預測\n",
    "    test_features = test_data.drop('price', axis=1)\n",
    "    submission['price'] = model.predict(test_features)\n",
    "    \n",
    "    # 儲存提交檔案\n",
    "    submission.to_csv('submission_RandomKFold.csv', index=False)\n",
    "    print(\"提交檔案已儲存為 submission_RandomKFold.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T15:14:53.846169Z",
     "iopub.status.busy": "2025-06-23T15:14:53.845994Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 倫敦房價預測 - 最終整合版 ===\n",
      "載入資料...\n",
      "創建時間特徵...\n",
      "處理地址特徵...\n",
      "進行高級地址特徵工程...\n",
      "開始處理缺失值...\n",
      "填補 floorAreaSqM 的缺失值（策略：most_frequent）...\n",
      "使用 floorAreaSqM 預測填補 bathrooms 的缺失值...\n",
      "使用 floorAreaSqM 預測填補 bedrooms 的缺失值...\n",
      "填補 livingRooms 的缺失值（策略：most_frequent）...\n",
      "填補 tenure 的缺失值（策略：most_frequent）...\n",
      "填補 propertyType 的缺失值（策略：most_frequent）...\n",
      "填補 currentEnergyRating 的缺失值（策略：most_frequent）...\n",
      "創建時間序列特徵...\n",
      "預測起點: 2023-12\n",
      "領先時間: 1 個月\n",
      "預測範圍: 7 個月\n",
      "創建額外特徵...\n",
      "-> 開始執行地理特徵工程（僅純幾何特徵）...\n",
      "-> 純地理特徵工程完成。\n",
      "\n",
      "--- 開始自動化特徵組合 ---\n",
      "-> 執行特徵組合...\n",
      "    初始化 NumPy 矩陣以加速相關性計算...\n",
      "    正在搜索最佳算術組合 (使用向量化)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    發現 307 個新組合特徵。\n",
      "-> 執行特徵組合...\n",
      "    應用 307 個已儲存的特徵組合。\n",
      "--- 自動化特徵組合完成 ---\n",
      "\n",
      "準備特徵集合...\n",
      "定義了 349 個機器學習特徵。\n",
      "\n",
      "=== 開始 Stacking 訓練 (1 種基礎模型) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    訓練 Fold 模型:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"主要執行流程（已整合所有特徵工程和 Stacking 架構）\"\"\"\n",
    "    print(\"=== 倫敦房價預測 - 最終整合版 ===\")\n",
    "    \n",
    "    # 1. 載入資料\n",
    "    train_df, test_df = load_and_prepare_data()\n",
    "\n",
    "    # --- 核心特徵工程管線 (步驟 2 到 7.5) ---\n",
    "    data_list = create_time_features([train_df, test_df])\n",
    "    train_df, test_df = data_list[0], data_list[1]\n",
    "    train_df = train_df.set_index('time')\n",
    "    test_df = test_df.set_index('time')\n",
    "    data_list = preprocess_address_features([train_df, test_df])\n",
    "    data_list = engineer_address_features(data_list)\n",
    "    train_df, test_df = data_list[0], data_list[1]\n",
    "    data_list = handle_missing_values([train_df, test_df])\n",
    "    train_df, test_df = data_list[0], data_list[1]\n",
    "    train_df, test_df, time_series_features = create_time_series_features(train_df, test_df)\n",
    "    data_list = create_additional_features([train_df, test_df])\n",
    "    train_df, test_df = data_list[0], data_list[1]\n",
    "    train_df, test_df = engineer_geo_features(train_df, test_df)\n",
    "    \n",
    "    print(\"\\n--- 開始自動化特徵組合 ---\")\n",
    "    feature_combo_config = {}\n",
    "    y_train_backup = train_df['price'].copy()\n",
    "    train_df, feature_combo_config = better_features(\n",
    "        features=train_df.drop(columns=['price']), \n",
    "        target_series=y_train_backup,\n",
    "        is_train=True,\n",
    "        config=feature_combo_config\n",
    "    )\n",
    "    train_df = train_df.copy()\n",
    "    train_df['price'] = y_train_backup\n",
    "    test_df, _ = better_features(\n",
    "        features=test_df, \n",
    "        target_series=None,\n",
    "        is_train=False,\n",
    "        config=feature_combo_config\n",
    "    )\n",
    "    print(\"--- 自動化特徵組合完成 ---\\n\")\n",
    "    \n",
    "    # 8. 準備最終特徵\n",
    "    X_train, y_train, trend_features, machine_learning_features, test_df = prepare_features(\n",
    "        train_df, test_df, time_series_features\n",
    "    )\n",
    "    \n",
    "    # 9. 【關鍵】: 在所有流程外部預先編碼\n",
    "    encoder = CustomEncoder()\n",
    "    X_train_encoded = encoder.fit_transform(X_train, y_train)\n",
    "    # test_df_encoded = encoder.transform(test_df)\n",
    "\n",
    "    # 10. 定義模型參數 (使用你找到的最佳參數)\n",
    "    lgbm_params = {\n",
    "        'n_estimators': 2500, 'max_depth': 7, 'learning_rate': 0.08840671722264341, \n",
    "        'subsample': 0.9097714385340092, 'colsample_bytree': 0.8595636633905605, \n",
    "        'reg_alpha': 4.2313700227253115, 'reg_lambda': 0.008959635134224476\n",
    "    }\n",
    "    \n",
    "    # 為 XGBoost 設定一組合理的參數 (可以用之前實驗的)\n",
    "    xgb_params = {\n",
    "        'n_estimators': 2500, 'max_depth': 7, 'learning_rate': 0.08840671722264341, \n",
    "        'subsample': 0.9097714385340092, 'colsample_bytree': 0.8595636633905605, \n",
    "        'reg_alpha': 4.2313700227253115, 'reg_lambda': 0.008959635134224476\n",
    "    }\n",
    "\n",
    "    # 11. 【新架C構】創建並訓練 Stacking 模型\n",
    "    \n",
    "    # 基礎模型 1: LightGBM\n",
    "    base_model_lgbm = Pipeline([\n",
    "            ('Encoder', CustomEncoder()),\n",
    "            ('Model', HybridModel(\n",
    "                trend_model=Ridge(alpha=0.1),\n",
    "                machine_model=LGBMRegressor(device='gpu', verbose=-1, **lgbm_params),\n",
    "                trend_cols=trend_features,\n",
    "                machine_cols=machine_learning_features,\n",
    "                all_columns=X_train.columns\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "    # 基礎模型 2: XGBoost\n",
    "    base_model_xgb = Pipeline([\n",
    "            ('Encoder', CustomEncoder()),\n",
    "            ('Model', HybridModel(\n",
    "                trend_model=Ridge(alpha=0.1),\n",
    "                machine_model=XGBRegressor(**xgb_params),\n",
    "                trend_cols=trend_features,\n",
    "                machine_cols=machine_learning_features,\n",
    "                all_columns=X_train.columns\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "    meta_model_instance = SummingRegressor()\n",
    "    final_model1 = StackingModel(\n",
    "        base_models=[\n",
    "            base_model_lgbm, \n",
    "            base_model_xgb],\n",
    "        meta_model=meta_model_instance,\n",
    "        n_splits=5\n",
    "    )\n",
    "    final_model1.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    print(\"\\n--- stacking模型評估 ---\")\n",
    "    evaluate_model(final_model1, X_train, y_train)\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"\\n--- 生成提交文件 ---\")\n",
    "    generate_submission(final_model1, test_df)\n",
    "    \n",
    "    print(\"\\n=== 程序執行完成 ===\")\n",
    "\n",
    "\n",
    "# 執行主程序\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7687677,
     "sourceId": 12204104,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7693306,
     "sourceId": 12212383,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
